{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZrQG0LeKK33+IoQl1w5Rt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeyanthan-gj/NLP-AND-LLM/blob/main/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Extraction in NLP\n",
        "**Feature extraction** is the process of transforming raw text into **numerical representations** that machine learning models can understand.  \n",
        "- Text data is **unstructured**, and models require **structured numeric input**.  \n",
        "- Feature extraction helps capture the **important information** from text in a usable form.  \n",
        "---\n",
        "\n",
        "##Common Types of Feature Extraction in NLP\n",
        "\n",
        "### 1ï¸âƒ£ N-grams\n",
        "\n",
        "### 2ï¸âƒ£ Bag of Words (BoW)\n",
        "### 3ï¸âƒ£ TF-IDF (Term Frequency â€“ Inverse Document Frequency)\n"
      ],
      "metadata": {
        "id": "Ems_i2K9N7Tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams\n",
        "**N-grams** are sequences of **N consecutive words** in a text.  \n",
        "\n",
        "## Types of N-grams\n",
        "| N | Name | Example (Sentence: â€œI like machine learningâ€) |\n",
        "|---|------|-----------------------------------------------|\n",
        "| 1 | Unigram | I, like, machine, learning |\n",
        "| 2 | Bigram | I like, like machine, machine learning |\n",
        "| 3 | Trigram | I like machine, like machine learning |\n",
        "\n",
        "> N can be any number depending on how much context you want.\n",
        "---\n",
        "\n",
        "## Steps to Create N-grams\n",
        "1. **Tokenize text** â†’ split sentence into words  \n",
        "2. **Slide a window of size N** over the tokens  \n",
        "3. **Collect all sequences** â†’ these are your N-grams  \n",
        "\n"
      ],
      "metadata": {
        "id": "cuhwD7C1DyUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "hu4K4F0sD-nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize text"
      ],
      "metadata": {
        "id": "rNXkAhm1NV9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I like machine learning\",\n",
        "    \"I like deep learning\"\n",
        "]\n",
        "\n",
        "# Tokenize documents\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "tokenized_docs\n"
      ],
      "metadata": {
        "id": "devQSjHyMGOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slide a window of size N"
      ],
      "metadata": {
        "id": "8owaUOhmNYHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 2  # Bigram example)(TRY WITH DIFF. VALUES OF N)\n",
        "bigrams_list = [list(ngrams(doc, N)) for doc in tokenized_docs]\n",
        "bigrams_list\n"
      ],
      "metadata": {
        "id": "tqs-PTp9MJEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect all sequences"
      ],
      "metadata": {
        "id": "uukuez48Na3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten and create a DataFrame for easy viewing\n",
        "import pandas as pd\n",
        "\n",
        "all_bigrams = [bigram for doc in bigrams_list for bigram in doc]\n",
        "df_bigrams = pd.DataFrame(all_bigrams, columns=[\"Word1\", \"Word2\"])\n",
        "df_bigrams\n"
      ],
      "metadata": {
        "id": "gm79FVOPMLVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words (BoW)\n",
        "Bag of Words (BoW) is a **text representation model** used in **Natural Language Processing (NLP)**.  \n",
        "It converts textual data into **numerical form** so that machine learning algorithms can process it.\n",
        "---\n",
        "## ðŸ”¹ Working Principle\n",
        "\n",
        "### Step 1: Collect Text Data\n",
        "Multiple documents or sentences are taken as input.\n",
        "\n",
        "### Step 2: Create Vocabulary\n",
        "All **unique words** from the entire dataset are listed to form a vocabulary.\n",
        "\n",
        "### Step 3: Vector Formation\n",
        "Each document is converted into a vector based on:\n",
        "- Presence of a word\n",
        "- Frequency of a word\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S1OO3bzB8SQy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mxW95Z-8J5f"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Download required tokenizer resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Collect Text Data"
      ],
      "metadata": {
        "id": "4SOO9wMJ8x4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I like machine learning\",\n",
        "    \"I like deep learning\"\n",
        "]\n",
        "\n",
        "documents\n"
      ],
      "metadata": {
        "id": "ugVCnQJB8yPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create Vocabulary"
      ],
      "metadata": {
        "id": "glXMRn1c85Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize documents\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Create vocabulary\n",
        "vocabulary = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "\n",
        "vocabulary\n"
      ],
      "metadata": {
        "id": "dNYep7R081yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Vector Formation"
      ],
      "metadata": {
        "id": "Hyblf73S9CXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectors = []\n",
        "\n",
        "for doc in tokenized_docs:\n",
        "    word_count = Counter(doc)\n",
        "    bow_vectors.append([word_count[word] for word in vocabulary])\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(bow_vectors, columns=vocabulary)\n",
        "bow_df\n"
      ],
      "metadata": {
        "id": "yV7a-qhH9Cqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF (Term Frequency â€“ Inverse Document Frequency)\n",
        "TF-IDF is a technique in **Natural Language Processing (NLP)** that converts text into numerical vectors.  \n",
        "It gives **more importance to meaningful words** and **reduces the impact of common words**.\n",
        "---\n",
        "## Why TF-IDF is Better than Bag of Words\n",
        "- BoW treats all words equally â†’ common words dominate  \n",
        "- TF-IDF **downweights common words** and **upweights rare but important words**  \n",
        "- Highlights **keywords and important terms** in documents  \n",
        "---\n"
      ],
      "metadata": {
        "id": "jGfANbOeAjOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Steps to Compute TF-IDF\n",
        "1. **Term Frequency (TF):** Count how often a word appears in a document  \n",
        "2. **Inverse Document Frequency (IDF):** Measure how rare a word is across all documents  \n",
        "3. **Multiply TF Ã— IDF:** Gives the **importance score** of a word in a document\n"
      ],
      "metadata": {
        "id": "AVMUjWQRAsvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "# Download tokenizer\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "Mzb_utFkAwUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 â€“ Term Frequency (TF)"
      ],
      "metadata": {
        "id": "psDvRwj-BVT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"I like machine learning\",\n",
        "    \"I like deep learning\",\n",
        "    \"Machine learning is fun\"\n",
        "]\n",
        "\n",
        "# Tokenize documents\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Compute TF\n",
        "vocabulary = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "tf_list = []\n",
        "for doc in tokenized_docs:\n",
        "    word_count = Counter(doc)\n",
        "    tf_list.append([word_count[word]/len(doc) for word in vocabulary])\n",
        "\n",
        "tf_list\n"
      ],
      "metadata": {
        "id": "gKLyEMDrA8rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 â€“ Inverse Document Frequency (IDF)"
      ],
      "metadata": {
        "id": "j3aNH-FBBXFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(documents)\n",
        "idf_list = []\n",
        "for word in vocabulary:\n",
        "    containing_docs = sum(1 for doc in tokenized_docs if word in doc)\n",
        "    idf_list.append(math.log(N / (1 + containing_docs)))  # add 1 to avoid division by zero\n",
        "\n",
        "idf_list\n"
      ],
      "metadata": {
        "id": "FwaRh1l_BBQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 â€“ TF Ã— IDF"
      ],
      "metadata": {
        "id": "-LTZvvfGBY0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectors = []\n",
        "for tf in tf_list:\n",
        "    tfidf_vectors.append([tf[i]*idf_list[i] for i in range(len(vocabulary))])\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_vectors, columns=vocabulary)\n",
        "tfidf_df\n"
      ],
      "metadata": {
        "id": "bzaOd7tgBD3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}